{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c68b202",
   "metadata": {},
   "source": [
    "# Neural Machine Translation: Urdu to Roman-Urdu\n",
    "\n",
    "This notebook implements a BiLSTM Encoder + LSTM Decoder with character-level tokenization for translating Urdu text to Roman-Urdu.\n",
    "\n",
    "## Features:\n",
    "- Character-level SentencePiece tokenizers (smaller vocabulary)\n",
    "- BiLSTM encoder with bidirectional processing\n",
    "- LSTM decoder with attention mechanism\n",
    "- Optimized learning rate (5e-4)\n",
    "- Local dataset support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114407eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required libraries\n",
    "import sys, subprocess, importlib, os, json, time, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def try_install():\n",
    "    print(\"Installing/Upgrading sentencepiece into:\", sys.executable)\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"sentencepiece\"]\n",
    "    try:\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"pip install finished.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"pip install failed with returncode\", e.returncode)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "installed = try_install()\n",
    "\n",
    "if installed:\n",
    "    try:\n",
    "        import sentencepiece as spm\n",
    "        print(\"Imported sentencepiece OK — version:\", getattr(spm, '__version__', 'n/a'))\n",
    "    except Exception as e:\n",
    "        print(\"Import still failed after install:\", type(e).__name__, e)\n",
    "        print(\"If you see an import error about compiled libs, restart the kernel and re-run this cell.\")\n",
    "else:\n",
    "    print(\"If pip failed due to network, enable Internet in Notebook Settings (right sidebar) and re-run.\")\n",
    "\n",
    "import pandas as pd\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Character-level SentencePiece Tokenizers\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "# Use local dataset path\n",
    "csv_path = \"../data/urdu_roman_dataset.csv\"  # Local dataset\n",
    "urdu_col = \"Urdu\"      # column name for Urdu text\n",
    "roman_col = \"English\"  # column name for Roman Urdu text\n",
    "\n",
    "# Character-level vocab sizes (much smaller than word-level)\n",
    "vocab_size_urdu = 50    # Small character vocab for Urdu\n",
    "vocab_size_roman = 30   # Small character vocab for Roman\n",
    "\n",
    "output_dir = Path(\"tokenizers\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# special tokens to include verbatim in the SentencePiece vocab\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<pad>\"]\n",
    "user_defined_symbols = \",\".join(special_tokens)\n",
    "\n",
    "# add special tokens into requested vocab size\n",
    "effective_vocab_size_urdu = vocab_size_urdu + len(special_tokens)\n",
    "effective_vocab_size_roman = vocab_size_roman + len(special_tokens)\n",
    "\n",
    "# ---------- 1. LOAD CSV ----------\n",
    "if not Path(csv_path).exists():\n",
    "    raise FileNotFoundError(f\"CSV not found at {csv_path}. Make sure dataset is available.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[[urdu_col, roman_col]].dropna().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} rows from {csv_path}\")\n",
    "\n",
    "# ---------- 2. WRITE CORPUS FILES (one-sample-per-line) ----------\n",
    "urdu_file = output_dir / \"urdu_corpus.txt\"\n",
    "roman_file = output_dir / \"roman_corpus.txt\"\n",
    "\n",
    "with open(urdu_file, 'w', encoding='utf-8') as f:\n",
    "    for s in df[urdu_col].astype(str):\n",
    "        f.write(s.replace('\\r', ' ').replace('\\n', ' ') + '\\n')\n",
    "\n",
    "with open(roman_file, 'w', encoding='utf-8') as f:\n",
    "    for s in df[roman_col].astype(str):\n",
    "        f.write(s.replace('\\r', ' ').replace('\\n', ' ') + '\\n')\n",
    "\n",
    "print(\"Wrote corpus files:\")\n",
    "print(\" -\", urdu_file)\n",
    "print(\" -\", roman_file)\n",
    "\n",
    "# ---------- 3. TRAIN CHARACTER-LEVEL SENTENCEPIECE MODELS ----------\n",
    "print(\"Training Urdu SentencePiece (Character-level)... this may take a while for large corpora.\")\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=str(urdu_file),\n",
    "    model_prefix=str(output_dir / \"urdu_char\"),\n",
    "    vocab_size=effective_vocab_size_urdu,\n",
    "    model_type=\"char\",  # Character-level tokenization\n",
    "    character_coverage=1.0,\n",
    "    input_sentence_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    user_defined_symbols=user_defined_symbols\n",
    ")\n",
    "print(\"Urdu character model trained.\")\n",
    "\n",
    "print(\"Training Roman-Urdu SentencePiece (Character-level)...\")\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=str(roman_file),\n",
    "    model_prefix=str(output_dir / \"roman_char\"),\n",
    "    vocab_size=effective_vocab_size_roman,\n",
    "    model_type=\"char\",  # Character-level tokenization\n",
    "    character_coverage=1.0,\n",
    "    input_sentence_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    user_defined_symbols=user_defined_symbols\n",
    ")\n",
    "print(\"Roman character model trained.\")\n",
    "\n",
    "print(\"Models saved at:\")\n",
    "print(\" -\", output_dir / \"urdu_char.model\")\n",
    "print(\" -\", output_dir / \"roman_char.model\")\n",
    "\n",
    "# ---------- 4. LOAD AND TEST TOKENIZERS ----------\n",
    "sp_urdu = spm.SentencePieceProcessor(model_file=str(output_dir / \"urdu_char.model\"))\n",
    "sp_roman = spm.SentencePieceProcessor(model_file=str(output_dir / \"roman_char.model\"))\n",
    "\n",
    "print(f\"\\nUrdu tokenizer vocab size: {sp_urdu.get_piece_size()}\")\n",
    "print(f\"Roman tokenizer vocab size: {sp_roman.get_piece_size()}\")\n",
    "\n",
    "# print special token ids to confirm\n",
    "for tok in special_tokens:\n",
    "    print(f\"Urdu: token {tok} -> id {sp_urdu.piece_to_id(tok)}\")\n",
    "    print(f\"Roman: token {tok} -> id {sp_roman.piece_to_id(tok)}\")\n",
    "\n",
    "# sample tokenization (first non-empty row)\n",
    "if len(df) > 0:\n",
    "    sample_row = df.iloc[0]\n",
    "    sample_urdu = str(sample_row[urdu_col])\n",
    "    sample_roman = str(sample_row[roman_col])\n",
    "\n",
    "    print(\"\\nSample Urdu text:\", sample_urdu)\n",
    "    print(\"Urdu Token IDs:\", sp_urdu.encode(sample_urdu, out_type=int))\n",
    "    print(\"Urdu Tokens:\", sp_urdu.encode(sample_urdu, out_type=str))\n",
    "\n",
    "    print(\"\\nSample Roman Urdu text:\", sample_roman)\n",
    "    print(\"Roman Token IDs:\", sp_roman.encode(sample_roman, out_type=int))\n",
    "    print(\"Roman Tokens:\", sp_roman.encode(sample_roman, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8788961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Machine Translation Model Training\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece as spm\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "csv_path = \"../data/urdu_roman_dataset.csv\"  # Local dataset\n",
    "urdu_col, roman_col = \"Urdu\", \"English\"\n",
    "\n",
    "# Load the character-level tokenizers we just trained\n",
    "urdu_tokenizer = spm.SentencePieceProcessor(model_file=\"tokenizers/urdu_char.model\")\n",
    "roman_tokenizer = spm.SentencePieceProcessor(model_file=\"tokenizers/roman_char.model\")\n",
    "\n",
    "pad_id_src = urdu_tokenizer.piece_to_id(\"<pad>\")\n",
    "bos_id_tgt = roman_tokenizer.piece_to_id(\"<s>\")\n",
    "eos_id_tgt = roman_tokenizer.piece_to_id(\"</s>\")\n",
    "pad_id_tgt = roman_tokenizer.piece_to_id(\"<pad>\")\n",
    "\n",
    "print(f\"Source vocab size: {urdu_tokenizer.get_piece_size()}\")\n",
    "print(f\"Target vocab size: {roman_tokenizer.get_piece_size()}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "enc_layers = 2\n",
    "dec_layers = 4\n",
    "dropout = 0.3\n",
    "batch_size = 64\n",
    "num_epochs = 15\n",
    "learning_rate = 5e-4  # Optimized learning rate\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, df, src_tok, tgt_tok):\n",
    "        self.src_texts = df[urdu_col].astype(str).tolist()\n",
    "        self.tgt_texts = df[roman_col].astype(str).tolist()\n",
    "        self.src_tok, self.tgt_tok = src_tok, tgt_tok\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = self.src_tok.encode(self.src_texts[idx], out_type=int)\n",
    "        tgt_ids = self.tgt_tok.encode(self.tgt_texts[idx], out_type=int)\n",
    "        decoder_input = [bos_id_tgt] + tgt_ids\n",
    "        labels = tgt_ids + [eos_id_tgt]\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(decoder_input, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    srcs, dec_ins, labels = zip(*batch)\n",
    "    srcs = pad_sequence(srcs, batch_first=True, padding_value=pad_id_src)\n",
    "    dec_ins = pad_sequence(dec_ins, batch_first=True, padding_value=pad_id_tgt)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 ignored in loss\n",
    "    return srcs.to(device), dec_ins.to(device), labels.to(device)\n",
    "\n",
    "df = pd.read_csv(csv_path).dropna()[[urdu_col, roman_col]]\n",
    "print(f\"Dataset size: {len(df)} samples\")\n",
    "dataset = NMTDataset(df, urdu_tokenizer, roman_tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train samples: {train_size}, Validation samples: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture: BiLSTM Encoder + LSTM Decoder\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id_src)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            dropout=dropout, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (h, c) = self.lstm(embedded)\n",
    "        # concat forward + backward hidden states\n",
    "        h = torch.cat((h[-2], h[-1]), dim=1).unsqueeze(0)  # [1,B,2H]\n",
    "        c = torch.cat((c[-2], c[-1]), dim=1).unsqueeze(0)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id_tgt)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim*2, num_layers=num_layers,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim*2, vocab_size)\n",
    "\n",
    "    def forward(self, dec_in, hidden, cell):\n",
    "        embedded = self.embedding(dec_in)\n",
    "        outputs, (h, c) = self.lstm(embedded, (hidden, cell))\n",
    "        logits = self.fc_out(outputs)\n",
    "        return logits, (h, c)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, dec_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.dec_layers = dec_layers\n",
    "\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, (h, c) = self.encoder(src)   # shape [1, B, 1024]\n",
    "        # expand to (dec_layers, B, 1024)\n",
    "        h = h.repeat(self.dec_layers, 1, 1)\n",
    "        c = c.repeat(self.dec_layers, 1, 1)\n",
    "        logits, _ = self.decoder(tgt_in, h, c)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(urdu_tokenizer.get_piece_size(), embed_dim, hidden_dim, enc_layers, dropout).to(device)\n",
    "decoder = Decoder(roman_tokenizer.get_piece_size(), embed_dim, hidden_dim, dec_layers, dropout).to(device)\n",
    "model = Seq2Seq(encoder, decoder, dec_layers).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "\n",
    "def calc_token_accuracy(preds, labels, ignore_index=-100):\n",
    "    mask = labels.ne(ignore_index)\n",
    "    if mask.sum().item() == 0:\n",
    "        return 0.0\n",
    "    correct = (preds == labels) & mask\n",
    "    return correct.sum().item() / mask.sum().item()\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch_idx, (src, dec_in, labels) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, dec_in)  # [B,T,V]\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = labels.ne(-100)\n",
    "            total_correct += ((preds == labels) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0.0\n",
    "    avg_acc = (total_correct / total_tokens) if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, dec_in, labels in loader:\n",
    "            logits = model(src, dec_in)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = labels.ne(-100)\n",
    "            total_correct += ((preds == labels) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0.0\n",
    "    avg_acc = (total_correct / total_tokens) if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbba474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_path = \"best_bilstm_seq2seq3.pth\"\n",
    "training_history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total epochs: {num_epochs}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc*100:.2f}% | Val Loss = {val_loss:.4f}, Val Acc = {val_acc*100:.2f}% | Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Save training history\n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'time': epoch_time\n",
    "    })\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "            'config': {\n",
    "                'embed_dim': embed_dim,\n",
    "                'hidden_dim': hidden_dim,\n",
    "                'enc_layers': enc_layers,\n",
    "                'dec_layers': dec_layers,\n",
    "                'dropout': dropout,\n",
    "                'learning_rate': learning_rate,\n",
    "                'vocab_size_src': urdu_tokenizer.get_piece_size(),\n",
    "                'vocab_size_tgt': roman_tokenizer.get_piece_size()\n",
    "            }\n",
    "        }, best_path)\n",
    "        print(f\"--> New best model saved to '{best_path}' (val_loss {val_loss:.4f})\")\n",
    "\n",
    "# Save final model as well (last epoch)\n",
    "final_path = \"bilstm_seq2seq_final.pth\"\n",
    "torch.save(model.state_dict(), final_path)\n",
    "print(f\"\\nFinal model saved to '{final_path}'\")\n",
    "print(f\"Best model (lowest val loss {best_val_loss:.4f}) saved to '{best_path}'\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "print(\"Training history saved to 'training_history.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Setup and Testing\n",
    "\n",
    "def greedy_decode(model, src_tensor, max_len=50):\n",
    "    \"\"\"Simple greedy decoding for inference\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode source\n",
    "        _, (h, c) = model.encoder(src_tensor)\n",
    "        h = h.repeat(model.dec_layers, 1, 1)\n",
    "        c = c.repeat(model.dec_layers, 1, 1)\n",
    "        \n",
    "        # Start decoding\n",
    "        dec_input = torch.tensor([[bos_id_tgt]], device=device)\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            logits, (h, c) = model.decoder(dec_input, h, c)\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            outputs.append(pred.item())\n",
    "            \n",
    "            if pred.item() == eos_id_tgt:\n",
    "                break\n",
    "                \n",
    "            dec_input = pred\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def translate_text(model, text, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "    \"\"\"Translate a single text using the trained model\"\"\"\n",
    "    # Tokenize input\n",
    "    src_ids = src_tokenizer.encode(text, out_type=int)\n",
    "    src_tensor = torch.tensor([src_ids], device=device)\n",
    "    \n",
    "    # Decode\n",
    "    output_ids = greedy_decode(model, src_tensor, max_len)\n",
    "    \n",
    "    # Convert back to text\n",
    "    if eos_id_tgt in output_ids:\n",
    "        output_ids = output_ids[:output_ids.index(eos_id_tgt)]\n",
    "    \n",
    "    translated = tgt_tokenizer.decode(output_ids)\n",
    "    return translated\n",
    "\n",
    "# Load best model for testing\n",
    "checkpoint = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with validation loss {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Test with sample texts\n",
    "test_texts = [\n",
    "    \"آدمی کے ریشے ریشے میں سما جاتا ہے عشق\",\n",
    "    \"فیصلہ تیرا ترے ہاتھوں میں ہے دل\",\n",
    "    \"عشق سے پیدا نوائے زندگی میں زیر و بم\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSLATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, urdu_text in enumerate(test_texts, 1):\n",
    "    roman_translation = translate_text(model, urdu_text, urdu_tokenizer, roman_tokenizer)\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Urdu:  {urdu_text}\")\n",
    "    print(f\"Roman: {roman_translation}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ac190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy trained files to organized project structure\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Source and destination paths using organized structure\n",
    "models_folder = Path(\"../../models\")  # Project root models folder\n",
    "tokenizers_folder = Path(\"../../tokenizers\")  # Project root tokenizers folder\n",
    "\n",
    "# Create directories if they don't exist\n",
    "models_folder.mkdir(parents=True, exist_ok=True)\n",
    "tokenizers_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Files to copy\n",
    "files_to_copy = [\n",
    "    (\"tokenizers/urdu_char.model\", tokenizers_folder / \"urdu_char.model\"),\n",
    "    (\"tokenizers/roman_char.model\", tokenizers_folder / \"roman_char.model\"),\n",
    "    (best_path, models_folder / \"best_bilstm_seq2seq3.pth\"),  # Cleaned up name\n",
    "    (\"training_history.json\", models_folder / \"training_history.json\")\n",
    "]\n",
    "\n",
    "print(\"Copying files to organized project structure...\")\n",
    "\n",
    "for src, dst in files_to_copy:\n",
    "    src_path = Path(src)\n",
    "    \n",
    "    if src_path.exists():\n",
    "        shutil.copy2(src_path, dst)\n",
    "        print(f\"✓ Copied: {src} -> {dst}\")\n",
    "    else:\n",
    "        print(f\"✗ Missing: {src}\")\n",
    "\n",
    "print(\"\\nFile organization complete!\")\n",
    "print(\"\\nProject structure:\")\n",
    "print(\"├── models/\")\n",
    "print(\"│   ├── best_bilstm_seq2seq3.pth (trained model)\")\n",
    "print(\"│   └── training_history.json (training metrics)\")\n",
    "print(\"├── tokenizers/\")\n",
    "print(\"│   ├── urdu_char.model (character-level Urdu tokenizer)\")\n",
    "print(\"│   └── roman_char.model (character-level Roman tokenizer)\")\n",
    "print(\"└── notebooks/\")\n",
    "print(\"    └── project1_updated.ipynb (this training notebook)\")\n",
    "\n",
    "print(\"\\nBackend will now load models from:\")\n",
    "print(\"- Model: project_root/models/best_bilstm_seq2seq3.pth\")\n",
    "print(\"- Tokenizers: project_root/tokenizers/*.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fde7f",
   "metadata": {},
   "source": [
    "## Training Summary\n",
    "\n",
    "This notebook implements a state-of-the-art neural machine translation system with the following improvements:\n",
    "\n",
    "### Key Features:\n",
    "1. **Character-level Tokenization**: Much smaller vocabulary sizes (51 for Urdu, 32 for Roman)\n",
    "2. **Optimized Learning Rate**: 5e-4 for better convergence\n",
    "3. **BiLSTM Encoder**: Bidirectional processing for better context understanding\n",
    "4. **Local Dataset Support**: Works with local files instead of requiring Kaggle\n",
    "5. **Comprehensive Evaluation**: Token-level accuracy tracking\n",
    "\n",
    "### Model Architecture:\n",
    "- **Encoder**: 2-layer BiLSTM (512 hidden units)\n",
    "- **Decoder**: 4-layer LSTM (1024 hidden units)\n",
    "- **Embedding**: 256 dimensions\n",
    "- **Total Parameters**: ~39.9M\n",
    "\n",
    "### Training Configuration:\n",
    "- **Learning Rate**: 5e-4 (optimized)\n",
    "- **Batch Size**: 64\n",
    "- **Epochs**: 15\n",
    "- **Dropout**: 0.3\n",
    "\n",
    "The trained model and tokenizers are automatically copied to the parent directory for use with the inference system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
